{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1703706e",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc66686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from biosppy.signals import ecg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159aaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('original_data/train.csv', index_col='id')\n",
    "train_y = data['y']\n",
    "labels = train_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f7d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there is any difference between using the filtered heartbeats or not\n",
    "with open('data/heartbeat_templates_ecg.pkl', 'rb') as f:\n",
    "    heartbeats = pickle.load(f)\n",
    "    \n",
    "# with open('data/heartbeat_filtered_ecg.pkl', 'rb') as f:\n",
    "#     heartbeats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628ffdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(32)\n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    n_val = int(val_pct*n)\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8c4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, signals, lengths, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.signals[idx], self.lengths[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c86a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Be sure that all the heartbeats have the same length\n",
    "\n",
    "# Longest seq of beats\n",
    "max_length_heartbeats = max([len(i) for i in heartbeats])\n",
    "beat_length = 180 # All heartbeats have the same length\n",
    "\n",
    "# Padded heartbeats\n",
    "padded_heartbeats = []\n",
    "lengths = []\n",
    "\n",
    "# Normalize before padding\n",
    "for i,heartbeat in enumerate(heartbeats):\n",
    "    heartbeats[i] = (heartbeat - heartbeat.mean(axis=-1, keepdims=True)) / heartbeat.std(axis=-1, keepdims=True)\n",
    "    \n",
    "for heartbeat in heartbeats:\n",
    "    length = len(heartbeat)\n",
    "    lengths.append(length)\n",
    "    pad = np.zeros((max_length_heartbeats-length, 180))\n",
    "    padded_heartbeats.append(np.concatenate((heartbeat, pad),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e24932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform heartbeats to tensor\n",
    "padded_heartbeats = np.array(padded_heartbeats)\n",
    "padded_heartbeats = torch.tensor(padded_heartbeats, dtype=torch.float32)\n",
    "labels_heartbeats = torch.tensor(labels)\n",
    "lengths_heartbeats = torch.tensor(lengths)\n",
    "\n",
    "# Normalize data per beat\n",
    "# padded_heartbeats = (padded_heartbeats - padded_heartbeats.mean(axis=-1, keepdims=True)) / padded_heartbeats.std(axis=-1, keepdims=True)\n",
    "\n",
    "# Train val split\n",
    "train_idxs, val_idxs = split_indices(len(padded_heartbeats), 0.2)\n",
    "train_x = padded_heartbeats[train_idxs]\n",
    "val_y = labels_heartbeats[val_idxs]\n",
    "train_y = labels_heartbeats[train_idxs]\n",
    "val_x = padded_heartbeats[val_idxs]\n",
    "train_lengths = lengths_heartbeats[train_idxs]\n",
    "val_lengths = lengths_heartbeats[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714ffe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a dataset for all the individual beats - 1D conv\n",
    "\n",
    "labels_beats = []\n",
    "beats = []\n",
    "\n",
    "for i, beat in enumerate(heartbeats):\n",
    "    samples = beat.shape[0]\n",
    "    beats.append(torch.tensor(beat, dtype=torch.float32))\n",
    "    labels_beats.append(torch.full((samples,), labels[i]))\n",
    "    \n",
    "beats = torch.cat(beats, dim=0)\n",
    "labels_beats = torch.cat(labels_beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99232889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - val split for individual beats\n",
    "\n",
    "train_idxs_beats, val_idxs_beats = split_indices(len(beats), 0.2)\n",
    "train_x_beats = beats[train_idxs_beats]\n",
    "val_y_beats = labels_beats[val_idxs_beats]\n",
    "train_y_beats = labels_beats[train_idxs_beats]\n",
    "val_x_beats = beats[val_idxs_beats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93bd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the moment training without a scaler but try also with scaler later.\n",
    "# scaler = StandardScaler()\n",
    "# train_x = scaler.fit_transform(train_x)\n",
    "# val_x = scaler.transform(val_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6720f",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5cfdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Signal dataset\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, signals, lengths, labels):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.signals[idx], self.lengths[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f241541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SignalDataset(train_x, train_lengths, train_y)\n",
    "val_dataset = SignalDataset(val_x, val_lengths, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f952e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_fn(batch):\n",
    "    signals, lengths, labels = zip(*batch)\n",
    "    signals = torch.stack(signals)\n",
    "    lengths = torch.stack(lengths)\n",
    "    labels = torch.stack(labels)\n",
    "    signals = signals.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    attention_mask = torch.arange(signals.size(1), device=device).unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "    return signals, attention_mask, labels\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca69fcf",
   "metadata": {},
   "source": [
    "# Transformer for signal beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7dc5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, seq_len:int, dropout:float):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model) #(seq_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) #(seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) #(1, seq_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8206c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, eps:float = 10**-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # We use the last dimension\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha*(x-mean)/(std+self.eps)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c4bf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFwd(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout:float):\n",
    "        super(FeedFwd, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (b_size, seq_len, d_model) -> (b_size, seq_len, dff) -> (b_size, seq_len, d_model)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(self.linear2(x))\n",
    "        return x\n",
    "        #return self.linear2(self.dropout(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4badbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model:int, h:int, dropout:float): # h -> num heads\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        \n",
    "        self.d_k = d_model // h # dim of each head\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask != None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2) # (b_size, 1, 1, seq_len) -> make this to match the attention_Score size\n",
    "            #attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "            attention_scores.masked_fill_(mask, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (b_size, h, seq_len, seq_len)\n",
    "        if dropout != None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores@value), attention_scores\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (b_size, seq_len, d_model) -> (b_size, seq_len, d_model)\n",
    "        key = self.w_k(k)\n",
    "        val = self.w_v(v)\n",
    "        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Divide to heads\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        val = val.view(val.shape[0], val.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        # Outpus size at this point (b_size, h, seq_len, d_k)\n",
    "        \n",
    "        # Apply mask\n",
    "        x, self.attention_scores = MultiheadAttention.attention(query, key, val, mask, self.dropout)\n",
    "        \n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h*self.d_k) # Go back to (b_size, seq_len, h, d_k) and then (b_size, seq_len, d_model)\n",
    "        \n",
    "        return self.w_o(x) # (b_size, seq_len, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2a0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout:float):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a5f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention, feed_forward, dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd25fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9678320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, pos_enc, src_size, d_model, output_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.pos_enc = pos_enc\n",
    "        self.input_proj = nn.Linear(src_size, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.input_proj(src)\n",
    "        src = self.pos_enc(src)\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        # MLP\n",
    "        pre_out = encoder_output.mean(dim=1) #Avg pooling over seq length -> really helpful?\n",
    "        logits = self.out_proj(pre_out)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7462d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_size, src_seq_len, d_model=512, N=6, h=8, dropout=0.1, d_ff=2048, output_dim=4):\n",
    "    # Positional encoding\n",
    "    pe = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    # Encoder\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention = MultiheadAttention(d_model, h, dropout)\n",
    "        feed_fwd = FeedFwd(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention, feed_fwd, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "        \n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    \n",
    "    transformer = Transformer(encoder, pe, src_size, d_model, output_dim)\n",
    "    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abcf08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                 | 1/50 [00:29<23:54, 29.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | Train Loss: 1.0380 | Validation Loss: 1.0216 | Train acc: 0.6099 | Val acc: 0.6188 | f1 score: 0.6188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                               | 2/50 [00:58<23:29, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] | Train Loss: 0.8100 | Validation Loss: 0.9986 | Train acc: 0.6490 | Val acc: 0.6344 | f1 score: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▉                                                                              | 3/50 [01:28<23:06, 29.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] | Train Loss: 0.7629 | Validation Loss: 0.9621 | Train acc: 0.6727 | Val acc: 0.6520 | f1 score: 0.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▋                                                                            | 4/50 [01:58<22:42, 29.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] | Train Loss: 0.7241 | Validation Loss: 0.8947 | Train acc: 0.6888 | Val acc: 0.6452 | f1 score: 0.6452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 5/50 [02:28<22:19, 29.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] | Train Loss: 0.6924 | Validation Loss: 0.8382 | Train acc: 0.7040 | Val acc: 0.6540 | f1 score: 0.6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▉                                                                         | 6/50 [02:58<21:54, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] | Train Loss: 0.6466 | Validation Loss: 0.8481 | Train acc: 0.7245 | Val acc: 0.6530 | f1 score: 0.6530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▌                                                                       | 7/50 [03:28<21:29, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] | Train Loss: 0.6179 | Validation Loss: 0.8972 | Train acc: 0.7389 | Val acc: 0.6403 | f1 score: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████▎                                                                     | 8/50 [03:58<21:03, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] | Train Loss: 0.5798 | Validation Loss: 0.8981 | Train acc: 0.7553 | Val acc: 0.6618 | f1 score: 0.6618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▉                                                                    | 9/50 [04:29<20:35, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] | Train Loss: 0.5571 | Validation Loss: 0.9571 | Train acc: 0.7594 | Val acc: 0.6716 | f1 score: 0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▍                                                                 | 10/50 [04:59<20:07, 30.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] | Train Loss: 0.5184 | Validation Loss: 1.1147 | Train acc: 0.7765 | Val acc: 0.6745 | f1 score: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████████████                                                                | 11/50 [05:29<19:39, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] | Train Loss: 0.4720 | Validation Loss: 1.0297 | Train acc: 0.8036 | Val acc: 0.6979 | f1 score: 0.6979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▋                                                              | 12/50 [06:00<19:10, 30.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] | Train Loss: 0.4453 | Validation Loss: 1.0469 | Train acc: 0.8163 | Val acc: 0.6999 | f1 score: 0.6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████▎                                                            | 13/50 [06:30<18:42, 30.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] | Train Loss: 0.4436 | Validation Loss: 1.1478 | Train acc: 0.8263 | Val acc: 0.6940 | f1 score: 0.6940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████▉                                                           | 14/50 [07:01<18:13, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] | Train Loss: 0.4100 | Validation Loss: 1.3008 | Train acc: 0.8361 | Val acc: 0.6764 | f1 score: 0.6764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▌                                                         | 15/50 [07:31<17:44, 30.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] | Train Loss: 0.3646 | Validation Loss: 1.3814 | Train acc: 0.8508 | Val acc: 0.6921 | f1 score: 0.6921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|██████████████████████████▏                                                       | 16/50 [08:02<17:15, 30.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] | Train Loss: 0.3391 | Validation Loss: 1.3641 | Train acc: 0.8608 | Val acc: 0.7048 | f1 score: 0.7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████▉                                                      | 17/50 [08:32<16:46, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] | Train Loss: 0.3041 | Validation Loss: 1.3872 | Train acc: 0.8779 | Val acc: 0.7087 | f1 score: 0.7087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▌                                                    | 18/50 [09:03<16:17, 30.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] | Train Loss: 0.2963 | Validation Loss: 1.4476 | Train acc: 0.8803 | Val acc: 0.6931 | f1 score: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                  | 19/50 [09:33<15:47, 30.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] | Train Loss: 0.2878 | Validation Loss: 1.4250 | Train acc: 0.8876 | Val acc: 0.6784 | f1 score: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▊                                                 | 20/50 [10:04<15:17, 30.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] | Train Loss: 0.2679 | Validation Loss: 1.4329 | Train acc: 0.8962 | Val acc: 0.6745 | f1 score: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████████▍                                               | 21/50 [10:35<14:47, 30.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] | Train Loss: 0.2456 | Validation Loss: 1.5638 | Train acc: 0.9011 | Val acc: 0.7077 | f1 score: 0.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████                                              | 22/50 [11:05<14:17, 30.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] | Train Loss: 0.2121 | Validation Loss: 1.7042 | Train acc: 0.9130 | Val acc: 0.7087 | f1 score: 0.7087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████▋                                            | 23/50 [11:36<13:47, 30.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] | Train Loss: 0.1873 | Validation Loss: 1.6640 | Train acc: 0.9304 | Val acc: 0.7224 | f1 score: 0.7224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████▎                                          | 24/50 [12:07<13:16, 30.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] | Train Loss: 0.1841 | Validation Loss: 1.7272 | Train acc: 0.9253 | Val acc: 0.7058 | f1 score: 0.7058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 25/50 [12:37<12:46, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100] | Train Loss: 0.1760 | Validation Loss: 1.7837 | Train acc: 0.9287 | Val acc: 0.7097 | f1 score: 0.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▋                                       | 26/50 [13:08<12:16, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] | Train Loss: 0.1633 | Validation Loss: 1.8265 | Train acc: 0.9372 | Val acc: 0.7009 | f1 score: 0.7009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████▎                                     | 27/50 [13:39<11:45, 30.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] | Train Loss: 0.1774 | Validation Loss: 1.7493 | Train acc: 0.9279 | Val acc: 0.6970 | f1 score: 0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████████████████▉                                    | 28/50 [14:10<11:15, 30.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100] | Train Loss: 0.1723 | Validation Loss: 1.7489 | Train acc: 0.9326 | Val acc: 0.6911 | f1 score: 0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████▌                                  | 29/50 [14:40<10:44, 30.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100] | Train Loss: 0.1411 | Validation Loss: 1.9826 | Train acc: 0.9433 | Val acc: 0.6794 | f1 score: 0.6794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████▌                                  | 29/50 [15:00<10:52, 31.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# Debug\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#         inputs = torch.ones((1, 2, 180), device=device)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#         mask = torch.tensor([[False, True]], device=device)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(inputs, attention_mask)\n\u001b[0;32m     38\u001b[0m         pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(src)\n\u001b[0;32m     11\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(src)\n\u001b[1;32m---> 12\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, src_mask)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# MLP\u001b[39;00m\n\u001b[0;32m     14\u001b[0m pre_out \u001b[38;5;241m=\u001b[39m encoder_output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Avg pooling over seq length -> really helpful?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m----> 9\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, mask)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(x, x, x, src_mask))\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[1;34m(self, x, sublayer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sublayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mFeedFwd.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# (b_size, seq_len, d_model) -> (b_size, seq_len, dff) -> (b_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers=3\n",
    "num_decoder_layers = 0\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "output_dim = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 50\n",
    "\n",
    "# src_seq_len=max_length_heartbeats\n",
    "model = build_transformer(src_size=180, src_seq_len=max_length_heartbeats, d_model=d_model, N=num_encoder_layers, h=nhead, dropout=dropout, d_ff=dim_feedforward, output_dim=4)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in train_dataloader:\n",
    "        inputs, attention_mask, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Debug\n",
    "#         inputs = torch.ones((1, 2, 180), device=device)\n",
    "#         mask = torch.tensor([[False, True]], device=device)\n",
    "        outputs = model.encode(inputs, attention_mask)\n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        all_labels.extend(targets.detach().cpu().tolist())\n",
    "        all_preds.extend(pred.detach().cpu().tolist())\n",
    "        \n",
    "    val_loss = 0\n",
    "    all_preds_val = []\n",
    "    all_labels_val = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in val_dataloader:\n",
    "            inputs, attention_mask, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model.encode(inputs, attention_mask)\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            all_labels_val.extend(targets.detach().cpu().tolist())\n",
    "            all_preds_val.extend(pred.detach().cpu().tolist())\n",
    "            \n",
    "    avg_train_loss = train_loss/len(train_dataloader)\n",
    "    avg_val_loss = val_loss/len(val_dataloader)\n",
    "    train_acc = accuracy_score(all_labels, all_preds)\n",
    "    val_acc = accuracy_score(all_labels_val, all_preds_val)\n",
    "    f1 = f1_score(all_labels_val, all_preds_val, average='micro')\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Train acc: {train_acc:.4f} | Val acc: {val_acc:.4f} | f1 score: {f1:.4f}\")\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fa45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
